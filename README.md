# Food Classification to Audio for the Blind 
### Team: Rachel Kiehne, Rodney Small, Rudra Shah
> ### Tufts Gordon Institute, EM-0213 Advanced Data Analytics, Spring 2022

---
## Problem Statement
#### By classifying food images using AI technology, we could use this to help train augmented reality glasses for the visually impairmented, such that device could auditorily inform its wearer of what they are going to consume.
---
To solve this problem, we will train the dataset to classify different types of food such that the response of the algorithm determines which classification exists. The parameters in which we have determined are based on the existing AI measures and qualifications that were established in the already present dataset (Food-101). 
<br><br>

While at first, we thought we could tackle the problem of food versus non-food images, we found that problem to be too monumental to tackle. By classifying the types of foods, it is just as important to the beginnings of our goal as we can establish commonalities amongst different images. 
<br><br>

#### *Why is this important?*
As mentioned previously, this project could be used to train smart glasses that when scanning food items could help individuals whom are visually impaired make executive decisions autonomously. This would alleviate the need for additional human assistive help.  


#### last-food-models files:
Too large to upload to github so available at google drive:
* food-min-training-1: https://drive.google.com/file/d/1zluAUTsWtJ_PqyFT9x5oAJZYsaSZu0g-/view?usp=sharing
* food-min-training-2: https://drive.google.com/file/d/10oYAv0ZPADZHMQi2bucBRzv2CnAUIPkO/view?usp=sharing

